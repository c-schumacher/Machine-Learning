{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up training and testing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('train10000.csv', header=None)\n",
    "y_train = pd.read_csv('train10000_label.csv', names=['Target'], squeeze=True)\n",
    "X_test = pd.read_csv('test10000.csv', header=None)\n",
    "y_test = pd.read_csv('test10000_label.csv', names=['Target'], squeeze=True)\n",
    "\n",
    "df_train = pd.concat([X_train, y_train], axis=1, ignore_index=False)\n",
    "df_test = pd.concat([X_test, y_test], axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highlighting the severely imbalanced target variable in the training data (will need to undersample or oversample for this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf4AAAGDCAYAAADK03I6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nST1i8EuS1CP/H1L5SfabUX/aAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcaa2be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure(figsize=(8,6))\n",
    "plt.hist(df_train['Target'],bins=np.arange(3)-0.5)\n",
    "plt.title(\"Imbalanced Target Class in Training Data\")\n",
    "plt.xlabel('Non-Customer (0) vs. Customer (1)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0,1])\n",
    "plt.yticks(np.arange(0,10000, 1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for any features which have missing values in their rows. These appear to be encoded as 999000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column:  0  - 999000 encoded missing values present\n",
      "Column:  2  - 999000 encoded missing values present\n",
      "Column:  5  - 999000 encoded missing values present\n",
      "Column:  6  - 999000 encoded missing values present\n",
      "Column:  7  - 999000 encoded missing values present\n",
      "Column:  8  - 999000 encoded missing values present\n",
      "Column:  9  - 999000 encoded missing values present\n",
      "Column:  12  - 999000 encoded missing values present\n",
      "Column:  14  - 999000 encoded missing values present\n",
      "Column:  27  - 999000 encoded missing values present\n",
      "Column:  28  - 999000 encoded missing values present\n"
     ]
    }
   ],
   "source": [
    "nm_cols = [] # list of columns indices for those w/o missing vals\n",
    "m_cols = [] # list of column indices which have missing vals\n",
    "for c_idx, series in df_train.iteritems():\n",
    "    if series.isnull().any():\n",
    "        print \"Column: \", c_idx, \" null values present\"\n",
    "        m_cols.append(c_idx)\n",
    "        continue\n",
    "    elif series.isin([999000]).any():\n",
    "        print \"Column: \", c_idx,\" - 999000 encoded missing values present\"\n",
    "        m_cols.append(c_idx)\n",
    "        continue\n",
    "    nm_cols.append(c_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For columns with missing values I am checking what percentage of the values are missing. If there is a high percentage of missing values it won't make sense to impute based on other values in that row. \n",
    "\n",
    "Additionally, it is most likely not in our best interest to impute for features which are binary, so that should be avoided (columns with three variables represent binary since the third value is the missing one). Columns with low variable count but greater than 3 are likely ordinal and will be manually inspected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0 \t: #vars = 3 \t\t52.11% missing\n",
      "Column: 2 \t: #vars = 8 \t\t24.38% missing\n",
      "Column: 5 \t: #vars = 3 \t\t24.02% missing\n",
      "Column: 6 \t: #vars = 5 \t\t43.13% missing\n",
      "Column: 7 \t: #vars = 10 \t\t70.73% missing\n",
      "Column: 8 \t: #vars = 10 \t\t30.24% missing\n",
      "Column: 9 \t: #vars = 4 \t\t54.31% missing\n",
      "Column: 12 \t: #vars = 19 \t\t77.04% missing\n",
      "Column: 14 \t: #vars = 3 \t\t52.11% missing\n",
      "Column: 27 \t: #vars = 2853 \t\t5.04% missing\n",
      "Column: 28 \t: #vars = 11 \t\t5.04% missing\n"
     ]
    }
   ],
   "source": [
    "for idx in m_cols:\n",
    "    percentages = df_train[idx].value_counts(normalize=True)\n",
    "    n_vars = len(percentages)\n",
    "    print \"Column:\",idx, \"\\t: #vars =\", n_vars, \"\\t\\t{0}% missing\".format(percentages[999000]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data appears to have several columns with missing values encoded as 999000. Many of these have a very high amount of missing data. I will make copies of the training dataset:\n",
    "- 1: raw data set\n",
    "- 2: raw data set with columns removed that have missing vals\n",
    "- 3: raw data set with impute values for columns missing less than 25% of their data\n",
    "\n",
    "- 1a: data set #1 + feature normalization to (0,1)\n",
    "- 2a: data set #2 + feature normalization to (0,1)\n",
    "- 3a: data set #3 + feature normalization to (0,1)\n",
    "\n",
    "- 1b: data set #1 + feature selection with feature importance from ExtraTreesClassifier\n",
    "- 2b: data set #2 + feature selection with feature importance from ExtraTreesClassifier\n",
    "- 3b: data set #3 + feature selection with feature importance from ExtraTreesClassifier\n",
    "\n",
    "- 1c: data set #1a + feature selection with feature importance from ExtraTreesClassifier\n",
    "- 2c: data set #2a + feature selection with feature importance from ExtraTreesClassifier\n",
    "- 3c: data set #3a + feature selection with feature importance from ExtraTreesClassifier\n",
    "\n",
    "Each of these datasets will be balanced using k-fold stratified sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(df_trn, df_tst, mvals='remove_cols', impute_strat='most_frequent',\n",
    "               normalize=True, feat_select=True):\n",
    "    # partitioning training and testing in order to preprocess\n",
    "    df_train = df_trn\n",
    "    df_test = df_tst\n",
    "    \n",
    "    X_train = df_train.iloc[:, 0:len(df_train.columns)-1]\n",
    "    y_train = df_train.iloc[:, len(df_train.columns)-1]\n",
    "    X_test = df_test.iloc[:, 0:len(df_test.columns)-1]\n",
    "    y_test = df_test.iloc[:, len(df_test.columns)-1]\n",
    "    \n",
    "    # deciding how to handle columns with missing values (encoded 999000)\n",
    "    # options are removing columns entirely, imputing for all, or imputing\n",
    "    # those with only a reasonable portion of missing data (<25%)\n",
    "    nm_cols = [] # list of columns indices for those w/o missing vals\n",
    "    m_cols = [] # list of column indices which have missing vals\n",
    "    for c_idx, series in X_train.iteritems():\n",
    "        if series.isnull().any():\n",
    "            m_cols.append(c_idx)\n",
    "            continue\n",
    "        elif series.isin([999000]).any():\n",
    "            m_cols.append(c_idx)\n",
    "            continue\n",
    "        nm_cols.append(c_idx)\n",
    "    \n",
    "    # either removing columns with missing values or imputing for those vals\n",
    "    if mvals != None:\n",
    "        if mvals == 'remove_cols':\n",
    "            X_train = X_train[nm_cols]\n",
    "            X_test = X_test[nm_cols]\n",
    "        elif mvals == 'impute':\n",
    "            for idx in m_cols:\n",
    "                if X_train[idx].value_counts(normalize=True)[999000] > 0.25:\n",
    "                    X_train.drop([idx], axis=1, inplace=True)\n",
    "                    X_test.drop([idx], axis=1, inplace=True)\n",
    "                else:\n",
    "                    X_train[idx].replace(999000.0, np.nan, inplace=True)\n",
    "                    X_test[idx].replace(999000.0, np.nan, inplace=True)\n",
    "\n",
    "            imputer = SimpleImputer(strategy=impute_strat) \n",
    "            imputer.fit(X_train, y_train)\n",
    "            X_train = pd.DataFrame(data=imputer.transform(X_train))\n",
    "            X_test = pd.DataFrame(data=imputer.transform(X_test))\n",
    "                    \n",
    "    # normalizing or not, being consistent with training and test\n",
    "    if normalize == True:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train = pd.DataFrame(data=scaler.transform(X_train))\n",
    "        X_test = pd.DataFrame(data=scaler.transform(X_test))\n",
    "    \n",
    "    # using feature selection based on feat importance from extratreesclassifer\n",
    "    if feat_select == True:\n",
    "        xtree = ExtraTreesClassifier(n_estimators=50)\n",
    "        clf = xtree.fit(X_train, y_train)\n",
    "        model = SelectFromModel(clf, prefit=True)\n",
    "        clf_cols = X_train.columns[model.get_support()]\n",
    "        X_train = pd.DataFrame(data=model.transform(X_train), \n",
    "                              columns=clf_cols)\n",
    "        X_test = pd.DataFrame(data=model.transform(X_test), \n",
    "                              columns=clf_cols)\n",
    "    \n",
    "    df_train = pd.concat([X_train, y_train], axis=1, ignore_index=False)\n",
    "    df_test = pd.concat([X_test, y_test], axis=1, ignore_index=False)\n",
    "    \n",
    "    return (df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Charlie\\Anaconda2\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "# primary three models\n",
    "df_train1, df_test1 = df_train, df_test\n",
    "\n",
    "output2 = preprocess(df_train, df_test, mvals='remove_cols', normalize=False,\n",
    "                     feat_select=False)\n",
    "df_train2, df_test2 = output2[0], output2[1]\n",
    "\n",
    "output3 = preprocess(df_train, df_test, mvals='impute', normalize=False,\n",
    "                     feat_select=False)\n",
    "df_train3, df_test3 = output3[0], output3[1]\n",
    "\n",
    "\n",
    "# subgroup a\n",
    "output1a = preprocess(df_train, df_test, mvals=None, normalize=True,\n",
    "                     feat_select=False)\n",
    "df_train1a, df_test1a = output1a[0], output1a[1]\n",
    "\n",
    "output2a = preprocess(df_train, df_test, mvals='remove_cols', normalize=True,\n",
    "                     feat_select=False)\n",
    "df_train2a, df_test2a = output2a[0], output2a[1]\n",
    "\n",
    "output3a = preprocess(df_train, df_test, mvals='impute', normalize=True,\n",
    "                     feat_select=False)\n",
    "df_train3a, df_test3a = output3a[0], output3a[1]\n",
    "\n",
    "\n",
    "# subgroup b\n",
    "output1b = preprocess(df_train, df_test, mvals=None, normalize=False,\n",
    "                     feat_select=True)\n",
    "df_train1b, df_test1b = output1b[0], output1b[1]\n",
    "\n",
    "output2b = preprocess(df_train, df_test, mvals='remove_cols', normalize=False,\n",
    "                     feat_select=True)\n",
    "df_train2b, df_test2b = output2b[0], output2b[1]\n",
    "\n",
    "output3b = preprocess(df_train, df_test, mvals='impute', normalize=False,\n",
    "                     feat_select=True)\n",
    "df_train3b, df_test3b = output3b[0], output3b[1]\n",
    "\n",
    "\n",
    "# subgroup c\n",
    "output1c = preprocess(df_train, df_test, mvals=None, normalize=True,\n",
    "                     feat_select=True)\n",
    "df_train1c, df_test1c = output1c[0], output1c[1]\n",
    "\n",
    "output2c = preprocess(df_train, df_test, mvals='remove_cols', normalize=True,\n",
    "                     feat_select=True)\n",
    "df_train2c, df_test2c = output2c[0], output2c[1]\n",
    "\n",
    "output3c = preprocess(df_train, df_test, mvals='impute', normalize=True,\n",
    "                     feat_select=True)\n",
    "df_train3c, df_test3c = output3c[0], output3c[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = [[df_train1, df_test1], [df_train2, df_test2],\n",
    "            [df_train3, df_test3], [df_train1a, df_test1a], \n",
    "            [df_train2a, df_test2a], [df_train3a, df_test3a], \n",
    "            [df_train1b, df_test1b], [df_train2b, df_test2b], \n",
    "            [df_train3b, df_test3b], [df_train1c, df_test1c], \n",
    "            [df_train2c, df_test2c], [df_train3c, df_test3c]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating copies of the dataset which use SMOTE to balance the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smote = SMOTE(ratio='minority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_SM, y_train_SM = smote.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_SM = pd.DataFrame(data=X_train_SM)\n",
    "y_train_SM = pd.DataFrame(data=y_train_SM)\n",
    "\n",
    "df_train_SM = pd.concat([X_train_SM, y_train_SM], axis=1, ignore_index=False)\n",
    "df_test_SM = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generating copies of the datasets based on the SMOTE over-balanced files\n",
    "# primary three models\n",
    "df_train_SM1, df_test_SM1 = df_train_SM, df_test_SM\n",
    "\n",
    "output2 = preprocess(df_train_SM, df_test_SM, mvals='remove_cols', \n",
    "                     normalize=False,feat_select=False)\n",
    "df_train_SM2, df_test_SM2 = output2[0], output2[1]\n",
    "\n",
    "output3 = preprocess(df_train_SM, df_test_SM, mvals='impute', \n",
    "                     normalize=False, feat_select=False)\n",
    "df_train_SM3, df_test_SM3 = output3[0], output3[1]\n",
    "\n",
    "\n",
    "# subgroup a\n",
    "output1a = preprocess(df_train_SM, df_test_SM, mvals=None, normalize=True,\n",
    "                     feat_select=False)\n",
    "df_train_SM1a, df_test_SM1a = output1a[0], output1a[1]\n",
    "\n",
    "output2a = preprocess(df_train_SM, df_test_SM, mvals='remove_cols', normalize=True,\n",
    "                     feat_select=False)\n",
    "df_train_SM2a, df_test_SM2a = output2a[0], output2a[1]\n",
    "\n",
    "output3a = preprocess(df_train_SM, df_test_SM, mvals='impute', normalize=True,\n",
    "                     feat_select=False)\n",
    "df_train_SM3a, df_test_SM3a = output3a[0], output3a[1]\n",
    "\n",
    "\n",
    "# subgroup b\n",
    "output1b = preprocess(df_train_SM, df_test_SM, mvals=None, normalize=False,\n",
    "                     feat_select=True)\n",
    "df_train_SM1b, df_test_SM1b = output1b[0], output1b[1]\n",
    "\n",
    "output2b = preprocess(df_train_SM, df_test_SM, mvals='remove_cols', normalize=False,\n",
    "                     feat_select=True)\n",
    "df_train_SM2b, df_test_SM2b = output2b[0], output2b[1]\n",
    "\n",
    "output3b = preprocess(df_train_SM, df_test_SM, mvals='impute', normalize=False,\n",
    "                     feat_select=True)\n",
    "df_train_SM3b, df_test_SM3b = output3b[0], output3b[1]\n",
    "\n",
    "\n",
    "# subgroup c\n",
    "output1c = preprocess(df_train_SM, df_test_SM, mvals=None, normalize=True,\n",
    "                     feat_select=True)\n",
    "df_train_SM1c, df_test_SM1c = output1c[0], output1c[1]\n",
    "\n",
    "output2c = preprocess(df_train_SM, df_test_SM, mvals='remove_cols', normalize=True,\n",
    "                     feat_select=True)\n",
    "df_train_SM2c, df_test_SM2c = output2c[0], output2c[1]\n",
    "\n",
    "output3c = preprocess(df_train_SM, df_test_SM, mvals='impute', normalize=True,\n",
    "                     feat_select=True)\n",
    "df_train_SM3c, df_test_SM3c = output3c[0], output3c[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets_SM = [[df_train_SM1, df_test_SM1], [df_train_SM2, df_test_SM2],\n",
    "            [df_train_SM3, df_test_SM3], [df_train_SM1a, df_test1a], \n",
    "            [df_train_SM2a, df_test_SM2a], [df_train_SM3a, df_test_SM3a], \n",
    "            [df_train_SM1b, df_test_SM1b], [df_train_SM2b, df_test_SM2b], \n",
    "            [df_train_SM3b, df_test_SM3b], [df_train_SM1c, df_test_SM1c], \n",
    "            [df_train_SM2c, df_test_SM2c], [df_train_SM3c, df_test_SM3c]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump([datasets,datasets_SM], open('datasets.p', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a testing pipeline for comparing decision trees and random forests. Using stratified k-fold on each models training set with 66/33 training/testing split and using the testing set as validation, across a grid search of parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DTpipe(df_train, df_test):\n",
    "    param_grid = {'criterion': ['gini'],\n",
    "                  'max_depth': [None],\n",
    "                  'min_samples_split': [50, 75, 100],\n",
    "                  'min_samples_leaf': [10, 20, 30],\n",
    "                  'max_features': ['sqrt', None],\n",
    "                  'random_state': [0],\n",
    "                  'class_weight': ['balanced']       \n",
    "    }\n",
    "   \n",
    "    X_train = df_train.iloc[:, 0:len(df_train.columns)-1]\n",
    "    y_train = df_train.iloc[:, len(df_train.columns)-1]\n",
    "    X_test = df_test.iloc[:, 0:len(df_test.columns)-1]\n",
    "    y_test = df_test.iloc[:, len(df_test.columns)-1]\n",
    "    \n",
    "    dt_clf = DecisionTreeClassifier()\n",
    "    grid_search = GridSearchCV(estimator=dt_clf, param_grid=param_grid,\n",
    "                               cv=3)\n",
    "       \n",
    "    # fitting to the training data with 3 cross-folds and best params\n",
    "    clf = grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # getting predictions on resubstitution and generalization\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "      \n",
    "    return (precision_score(y_train, y_train_pred),\n",
    "            recall_score(y_train, y_train_pred),\n",
    "            grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DT_results = [DTpipe(x[0], x[1]) for x in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DT_results_SM = [DTpipe(x[0], x[1]) for x in datasets_SM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump([DT_results, DT_results_SM], open('DT_res.p', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RFpipe(df_train, df_test):\n",
    "    param_grid = {'n_estimators': [100],\n",
    "                  'criterion': ['gini'],\n",
    "                  'max_depth': [None],\n",
    "                  'min_samples_split': [50, 100],\n",
    "                  'min_samples_leaf': [25, 50],\n",
    "                  'max_features': ['sqrt'],\n",
    "                  'random_state': [0],\n",
    "                  'class_weight': ['balanced']      \n",
    "    }\n",
    "   \n",
    "    X_train = df_train.iloc[:, 0:len(df_train.columns)-1]\n",
    "    y_train = df_train.iloc[:, len(df_train.columns)-1]\n",
    "    X_test = df_test.iloc[:, 0:len(df_test.columns)-1]\n",
    "    y_test = df_test.iloc[:, len(df_test.columns)-1]\n",
    "    \n",
    "    rf_clf = RandomForestClassifier()\n",
    "    grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid,\n",
    "                               cv=3)\n",
    "       \n",
    "    # fitting to the training data with 3 cross-folds and best params\n",
    "    clf = grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # getting predictions on resubstitution and generalization\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "      \n",
    "    return (precision_score(y_train, y_train_pred),\n",
    "            recall_score(y_train, y_train_pred),\n",
    "            grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RF_results = [RFpipe(x[0], x[1]) for x in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF_results_SM = [RFpipe(x[0], x[1]) for x in datasets_SM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump([RF_results, RF_results_SM], open('RF_res.p', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9968284947548183 0.8989110108898911\n",
      "0.9957322277771004 0.898251017489825\n",
      "0.9970667318504033 0.8973710262897371\n",
      "0.995376566492274 0.8999010009899902\n",
      "0.9957322277771004 0.898251017489825\n",
      "0.9964625518419127 0.8985810141898581\n",
      "0.9969332679097154 0.8939610603893962\n",
      "0.9975363389997536 0.8907710922890771\n",
      "0.9983996060568755 0.8920910790892091\n",
      "0.9963275798751378 0.8952810471895281\n",
      "0.9974191962639793 0.8927510724892751\n",
      "0.9960721738063091 0.8926410735892641\n"
     ]
    }
   ],
   "source": [
    "for i in RF_results_SM:\n",
    "    print i[0], i[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating each of the Decision Tree and Random Forest models on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generalize(datasets, resub_results):\n",
    "    test_results = []\n",
    "    \n",
    "    for i in range(0, len(datasets)):\n",
    "        df_trn, df_tst = datasets[i][0], datasets[i][1]\n",
    "        \n",
    "        X_trn = df_trn.iloc[:, 0:len(df_trn.columns)-1]\n",
    "        y_trn = df_trn.iloc[:, len(df_trn.columns)-1]\n",
    "        X_tst = df_tst.iloc[:, 0:len(df_tst.columns)-1]\n",
    "        y_tst = df_tst.iloc[:, len(df_tst.columns)-1]\n",
    "        \n",
    "        clf = resub_results[i][2]\n",
    "        clf.fit(X_trn, y_trn)\n",
    "        y_pred = clf.predict(X_tst)\n",
    "        \n",
    "        test_results.append([precision_score(y_tst, y_pred),\n",
    "                            recall_score(y_tst, y_pred),\n",
    "                            f1_score(y_tst, y_pred),\n",
    "                            accuracy_score(y_tst, y_pred),\n",
    "                            confusion_matrix(y_tst, y_pred)])\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DT_test_results = generalize(datasets, DT_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DT_test_results_SM = generalize(datasets_SM, DT_results_SM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF_test_results = generalize(datasets, RF_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF_test_results_SM = generalize(datasets_SM, RF_results_SM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1251798561151079 0.3706070287539936 0.1871470825490723 0.6977\n",
      "0.11891891891891893 0.32800851970181044 0.17455369793142536 0.7087\n",
      "0.1197289156626506 0.33865814696485624 0.1769123783031989 0.7041\n",
      "0.1251798561151079 0.3706070287539936 0.1871470825490723 0.6977\n",
      "0.11891891891891893 0.32800851970181044 0.17455369793142536 0.7087\n",
      "0.1197289156626506 0.33865814696485624 0.1769123783031989 0.7041\n",
      "0.12102653834937299 0.44195953141640043 0.19001831501831504 0.6462\n",
      "0.1149298872697278 0.44515441959531415 0.1826923076923077 0.626\n",
      "0.11236288170767862 0.4036208732694356 0.17578849721706863 0.6446\n",
      "0.11603570329332102 0.4014909478168264 0.18003820439350524 0.6566\n",
      "0.10787716955941255 0.43024494142705005 0.17250213492741248 0.6124\n",
      "0.10637103564412012 0.4036208732694356 0.16836961350510884 0.6256\n"
     ]
    }
   ],
   "source": [
    "for i in DT_test_results:\n",
    "    print i[0], i[1], i[2], i[3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
